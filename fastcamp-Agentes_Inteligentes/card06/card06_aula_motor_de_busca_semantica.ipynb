{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto: Construindo um mecanismo de busca semântica"
      ],
      "metadata": {
        "id": "nSd0nA-EHKfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um motor de pesquisa semântica funcional que demonstra:\n",
        "\n",
        "- Conhecimento especializado: escolha conteúdos que compreende para poder avaliar a qualidade da pesquisa\n",
        "- Comparação de fragmentação: teste diferentes estratégias e veja qual funciona melhor para o seu tipo de conteúdo\n",
        "- Compreensão semântica real: pesquise por conceito, tema ou significado, em vez de palavras-chave exatas\n",
        "- Informações práticas: descubra o que torna a fragmentação eficaz na sua área específica"
      ],
      "metadata": {
        "id": "Qs_0W4rJIyxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers transformers qdrant-client llama-index-core llama-index-embeddings-huggingface -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cshAMNQHgBHP",
        "outputId": "3bd1e40a-a76c-41d6-8a09-c45032e7b3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient, models\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/qdrant.env') # Carrega variáveis do arquivo .env\n",
        "\n",
        "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
        "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
        "\n",
        "client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
        "\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhZgxklakjpN",
        "outputId": "7903cad9-4f04-4718-bae7-36c4c0d4a4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso! URL carregada: https://62f4713c-61df-4137-ab35-d40676003640.us-east4-0.gcp.cloud.qdrant.io:6333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset = [\n",
        "    {\n",
        "        \"title\": \"Projeto Holhooja\",\n",
        "        \"description\": \"\"\"O projeto concentra suas investigações na concepção de uma solução automatizada que\n",
        "        aborde as tecnologias da Internet das Coisas (IoT) e na elaboração de um sistema online\n",
        "        direcionado ao controle de acesso de ambientes laboratoriais. Este protótipo do sistema\n",
        "        integrado foi desenvolvido para o Laboratório de Práticas Autônomas (LPA) e tem como\n",
        "        principal atribuição a administração do registro de acesso do LPA, promovendo aprimoramentos\n",
        "        em termos de praticidade e controle do uso, ao mesmo tempo em que realiza\n",
        "        a verificação do tempo de permanência de cada um dos alunos cadastrados, facilitando\n",
        "        a geração de certificados de horas complementares. Paralelamente ao sistema de controle\n",
        "        de acesso, o escopo do projeto abrange a implementação de um sistema de automação\n",
        "        no laboratório. Este sistema, em conjunto com o controle de acesso, visa disponibilizar a\n",
        "        abertura do ambiente, gerenciar os dispositivos de ar condicionado e iluminação, forne-\n",
        "        cendo informações em tempo real sobre as condições do laboratório.\"\"\",\n",
        "        \"area\": \"IoT\",\n",
        "        \"subarea\": \"Automação\",\n",
        "        \"Nível\": \"Graduação\",\n",
        "        \"Instituição\": \"IFTO\"\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "4-MKMMbYJwmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_size_chunks(text, chunk_size=100, overlap=20):\n",
        "    \"\"\"Split text into fixed-size chunks with overlap\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        if chunk_words:  # Only add non-empty chunks\n",
        "            chunks.append(' '.join(chunk_words))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def sentence_chunks(text, max_sentences=3):\n",
        "    \"\"\"Group sentences into chunks\"\"\"\n",
        "    import re\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), max_sentences):\n",
        "        chunk_sentences = sentences[i:i + max_sentences]\n",
        "        if chunk_sentences:\n",
        "            chunks.append('. '.join(chunk_sentences) + '.')\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def paragraph_chunks(text):\n",
        "    \"\"\"Split by paragraphs or double line breaks\"\"\"\n",
        "    chunks = [chunk.strip() for chunk in text.split('\\n\\n') if chunk.strip()]\n",
        "    return chunks if chunks else [text]  # Fallback to full text"
      ],
      "metadata": {
        "id": "aEIS9VI_Qn0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"day1_semantic_search\"\n",
        "\n",
        "if client.collection_exists(collection_name=collection_name):\n",
        "    client.delete_collection(collection_name=collection_name)\n",
        "\n",
        "# Ccria uma coleção com 3 vetores nomeados, cada um deles implementando uma estratégia de segmentação\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\n",
        "        \"fixed\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "        \"sentence\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "        \"paragraph\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "    },\n",
        ")\n",
        "\n",
        "# indexa os campos para filtragem (metadados)\n",
        "client.create_payload_index(\n",
        "    collection_name=collection_name,\n",
        "    field_name=\"chunk_strategy\",\n",
        "    field_schema=models.PayloadSchemaType.KEYWORD,\n",
        ")\n",
        "\n",
        "points = []\n",
        "point_id = 0\n",
        "\n",
        "for item in my_dataset:\n",
        "    description = item[\"description\"]\n",
        "\n",
        "    # processa cada uma das estratégias de segmentação\n",
        "    strategies = {\n",
        "        \"fixed\": fixed_size_chunks(description),\n",
        "        \"sentence\": sentence_chunks(description),\n",
        "        \"paragraph\": paragraph_chunks(description),\n",
        "    }\n",
        "\n",
        "    for strategy_name, chunks in strategies.items():\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            # Ccria o vetor desse segmento\n",
        "            vectors = {strategy_name: encoder.encode(chunk).tolist()}\n",
        "\n",
        "            points.append(\n",
        "                models.PointStruct(\n",
        "                    id=point_id,\n",
        "                    vector=vectors,\n",
        "                    payload={\n",
        "                        **item,  # inclui metadados originais\n",
        "                        \"chunk\": chunk,\n",
        "                        \"chunk_strategy\": strategy_name,\n",
        "                        \"chunk_index\": chunk_idx,\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "            point_id += 1\n",
        "\n",
        "client.upload_points(collection_name=collection_name, points=points)\n",
        "print(f\"Uploaded {len(points)} chunks across three strategies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Q6Mcp6Q7B8",
        "outputId": "b7fbfd29-2fa0-429f-8ae4-6a900e02ca4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded 5 chunks across three strategies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como entrar na sala de estudos especializada\n",
        "\n",
        "Monitoramento de quem usa a sala e por quanto tempo.\n",
        "\n",
        "Ligar luzes e climatização de forma remota.\n",
        "\n",
        "Geração automática de documentos de atividades extracurriculares.\n",
        "\n",
        "Solução para administrar o uso de um espaço compartilhado.\n",
        "\n",
        "O que o sistema faz quando alguém chega?"
      ],
      "metadata": {
        "id": "25zegfpcqaDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_search_results(query):\n",
        "    \"\"\"Comparando resultados\"\"\"\n",
        "    print(f\"Query: '{query}'\\n\")\n",
        "\n",
        "    for strategy in [\"fixed\", \"sentence\", \"paragraph\"]:\n",
        "        results = client.query_points(\n",
        "            collection_name=collection_name,\n",
        "            query=encoder.encode(query).tolist(),\n",
        "            using=strategy,\n",
        "            limit=3,\n",
        "        )\n",
        "\n",
        "        print(f\"--- {strategy.upper()} CHUNKING ---\")\n",
        "        for i, point in enumerate(results.points, 1):\n",
        "            print(f\"{i}. {point.payload['title']}\")\n",
        "            print(f\"   Score: {point.score:.3f}\")\n",
        "            print(f\"   Chunk: {point.payload['chunk'][:80]}...\")\n",
        "        print()\n",
        "\n",
        "\n",
        "test_queries = [\n",
        "    \"Como entrar na sala de estudos\",\n",
        "    \"Ligar luzes e climatização de forma remota\",\n",
        "    \"Solução para administrar o uso de um espaço compartilhado.\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    compare_search_results(query)"
      ],
      "metadata": {
        "id": "6xHueDARRmf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f29e17-3194-4026-84c3-e67af0689e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: 'Como entrar na sala de estudos'\n",
            "\n",
            "--- FIXED CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.429\n",
            "   Chunk: do tempo de permanência de cada um dos alunos cadastrados, facilitando a geração...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.348\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "--- SENTENCE CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.466\n",
            "   Chunk: Este sistema, em conjunto com o controle de acesso, visa disponibilizar a\n",
            "      ...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.358\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "--- PARAGRAPH CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.358\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "Query: 'Ligar luzes e climatização de forma remota'\n",
            "\n",
            "--- FIXED CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.286\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.262\n",
            "   Chunk: do tempo de permanência de cada um dos alunos cadastrados, facilitando a geração...\n",
            "\n",
            "--- SENTENCE CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.270\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.212\n",
            "   Chunk: Este sistema, em conjunto com o controle de acesso, visa disponibilizar a\n",
            "      ...\n",
            "\n",
            "--- PARAGRAPH CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.270\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "Query: 'Solução para administrar o uso de um espaço compartilhado.'\n",
            "\n",
            "--- FIXED CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.532\n",
            "   Chunk: do tempo de permanência de cada um dos alunos cadastrados, facilitando a geração...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.443\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "--- SENTENCE CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.538\n",
            "   Chunk: Este sistema, em conjunto com o controle de acesso, visa disponibilizar a\n",
            "      ...\n",
            "2. Projeto Holhooja\n",
            "   Score: 0.437\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n",
            "--- PARAGRAPH CHUNKING ---\n",
            "1. Projeto Holhooja\n",
            "   Score: 0.437\n",
            "   Chunk: O projeto concentra suas investigações na concepção de uma solução automatizada ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_chunking_effectiveness():\n",
        "    print(\"Análise das estratégias de segmentação\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Get chunk statistics for each strategy\n",
        "    for strategy in [\"fixed\", \"sentence\", \"paragraph\"]:\n",
        "        # Count chunks per strategy\n",
        "        results = client.scroll(\n",
        "            collection_name=collection_name,\n",
        "            scroll_filter=models.Filter(\n",
        "                must=[\n",
        "                    models.FieldCondition(\n",
        "                        key=\"chunk_strategy\", match=models.MatchValue(value=strategy)\n",
        "                    )\n",
        "                ]\n",
        "            ),\n",
        "            limit=100,\n",
        "        )\n",
        "\n",
        "        chunks = results[0]\n",
        "        chunk_sizes = [len(chunk.payload[\"chunk\"]) for chunk in chunks]\n",
        "\n",
        "        print(f\"\\n{strategy.upper()} STRATEGY:\")\n",
        "        print(f\"  Total de segmentações: {len(chunks)}\")\n",
        "        print(f\"  Tamanho médio da segmentação: {sum(chunk_sizes)/len(chunk_sizes):.0f} chars\")\n",
        "        print(f\"  Tamanho do intervalo: {min(chunk_sizes)}-{max(chunk_sizes)} chars\")\n",
        "\n",
        "\n",
        "analyze_chunking_effectiveness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqrWWfVjql7h",
        "outputId": "351f821a-1040-4d4e-a79c-7ad551c654ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Análise das estratégias de segmentação\n",
            "========================================\n",
            "\n",
            "FIXED STRATEGY:\n",
            "  Total de segmentações: 2\n",
            "  Tamanho médio da segmentação: 574 chars\n",
            "  Tamanho do intervalo: 485-664 chars\n",
            "\n",
            "SENTENCE STRATEGY:\n",
            "  Total de segmentações: 2\n",
            "  Tamanho médio da segmentação: 549 chars\n",
            "  Tamanho do intervalo: 244-854 chars\n",
            "\n",
            "PARAGRAPH STRATEGY:\n",
            "  Total de segmentações: 1\n",
            "  Tamanho médio da segmentação: 1099 chars\n",
            "  Tamanho do intervalo: 1099-1099 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDrQGaV3rImR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}