{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H40V0CnklJYV",
        "09ABewWJ3_cu",
        "wLcZlamk4CE7",
        "Y3-e7M0_q8MM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeMsgJ6t4omo",
        "outputId": "e5c2bc97-5086-4d10-8b88-ce155c641a41",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.36.0\n"
          ]
        }
      ],
      "source": [
        "%pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versão do video\n"
      ],
      "metadata": {
        "id": "H40V0CnklJYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY'] = \"GROQ_API_KEY\"\n"
      ],
      "metadata": {
        "id": "17XxShzi6jr8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    input=\"Explain the importance of fast language models\",\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        ")\n",
        "print(response.output_text)\n"
      ],
      "metadata": {
        "id": "s7EITTTI46Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, client, system):\n",
        "        self.client = client\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "        if self.system is not None:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
        "\n",
        "    def __call__(self, message=\"\"):\n",
        "        if message:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.execute()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "\n",
        "    def execute(self):\n",
        "        completion = self.client.chat.completions.create(\n",
        "            messages=self.messages,\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "        )\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "Q7gAdQ146ky7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "wikipedia:\n",
        "\n",
        "e.g. wikipedia: Django\n",
        "\n",
        "Returns a summary from searching Wikipedia\n",
        "\n",
        "simon_blog_search:\n",
        "\n",
        "e.g. simon_blog_search: Django\n",
        "\n",
        "Search Simon's blog for that term\n",
        "Always look things up on Wikipedia if you have the opportunity to do so.\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the capital of France?\n",
        "Thought: I should look up France on Wikipedia\n",
        "Action: wikipedia: France\n",
        "\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: France is a country. The capital is Paris.\n",
        "\n",
        "You then output:\n",
        "Answer: The capital of France is Paris\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "ZV5fmyeqlhHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(operation):\n",
        "    return eval(operation)\n",
        "\n",
        "def get_plamet_mass(planet) -> float:\n",
        "    match planet.lower():\n",
        "        case \"earth\":\n",
        "            return 5.972e24\n",
        "        case \"mars\":\n",
        "            return 6.39e23\n",
        "        case \"jupiter\":\n",
        "            return 1.898e27\n",
        "        case \"saturn\":\n",
        "            return 5.683e26\n",
        "        case \"uranus\":\n",
        "            return 8.681e25\n",
        "        case \"neptune\":\n",
        "            return 1.024e26\n",
        "        case \"mercury\":\n",
        "            return 3.301e23\n",
        "        case \"venus\":\n",
        "            return 4.867e24\n",
        "        case \"sun\":\n",
        "            return 1.989e30"
      ],
      "metadata": {
        "id": "PR0QdhZI_rxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neil_tyson = Agent(client, system_prompt)"
      ],
      "metadata": {
        "id": "MydFJl4NAeh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = neil_tyson(\"What is the mass of the sun times 2?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "ezXiWB9HP3dy",
        "outputId": "f9ba37e9-939d-42f9-dd3a-17239e413ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'Tool choice is none, but model called a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{\"name\": \"python\", \"arguments\": # no code yet}'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3059989349.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneil_tyson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the mass of the sun times 2?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-899617487.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-899617487.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         completion = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai/gpt-oss-20b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1188\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Tool choice is none, but model called a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{\"name\": \"python\", \"arguments\": # no code yet}'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def loop(max_iterations=10, query: str = \"\"):\n",
        "    agent = Agent(client=client, system=system_prompt)\n",
        "    tools = [\"calculate\", \"get_planet_mass\"]\n",
        "    next_prompt = query\n",
        "    i = 0\n",
        "    while i < max_iterations:\n",
        "        i += 1\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "\n",
        "        if \"PAUSE\" in result and \"Action\" in result:\n",
        "            action = re.findall(r\"Action: ([a-z_]+): (.+)\", result, re.IGNORECASE)\n",
        "            chosen_tool = action[0][0]\n",
        "            arg = action[0][1]\n",
        "\n",
        "            if chosen_tool in tools:\n",
        "                result_tool = eval(f\"{chosen_tool}('{arg}')\")\n",
        "                next_prompt = f\"Observation: {result_tool}\"\n",
        "\n",
        "            else:\n",
        "                next_prompt = \"Observation: Tool not found\"\n",
        "\n",
        "            print(next_prompt)\n",
        "            continue\n",
        "\n",
        "        if \"Answer\" in result:\n",
        "            break\n",
        "\n",
        "loop(query=\"What is the mass of Earth plus the mass of Saturn and all of that times 2?\")"
      ],
      "metadata": {
        "id": "1obUjrSo2ODN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrigindo a versão do vídeo"
      ],
      "metadata": {
        "id": "s5Pd2N7Ml_qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devido a evolução dos modelo e das bibliotecas usadas, parece que houve um descompasso entre as instruções dadas (*system_prompt*) ao modelo e a configuração do código apresentadas no vídeo.\n",
        "\n",
        "1. inicialmente o erro parece indicar uma falha de *parsing*, que acontece devido ao uso do prompt de ReAct sem ter um script que fizesse a formatação correta das requisições;\n",
        "2. posteriormente, o erro *BadRequest* surgiu quando o modelo tentou utilizar o recurso nativo de chamada de funções, mas foi bloqueado pela API porque as ferramentas (*tools*) não haviam sido declaradas formalmente na requisição, criando uma incompatibilidade entre a intenção do modelo e os parâmetros fornecidos."
      ],
      "metadata": {
        "id": "lP_3I3-bm66C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "BNPBgnflmj0T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(expression):\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Erro: {e}\"\n",
        "\n",
        "# função add para teste\n",
        "def wikipedia_search(query):\n",
        "    return f\"Resultado simulado da Wikipedia para: {query}\"\n",
        "\n",
        "def get_planet_mass(planet):\n",
        "    masses = {\n",
        "        \"earth\": 5.972e24,\n",
        "        \"mars\": 6.39e23,\n",
        "        \"jupiter\": 1.898e27,\n",
        "        \"saturn\": 5.683e26,\n",
        "        \"uranus\": 8.681e25,\n",
        "        \"neptune\": 1.024e26,\n",
        "        \"mercury\": 3.301e23,\n",
        "        \"venus\": 4.867e24,\n",
        "        \"sun\": 1.989e30\n",
        "    }\n",
        "    return str(masses.get(planet.lower(), \"Massa desconhecida\"))\n"
      ],
      "metadata": {
        "id": "B4IFgdVEqJko"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log criando manualmente pois não consegui acessar modelo.messages\n",
        "def save_log_to_json(messages, filename=\"historico_conversa.json\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(messages, f, indent=4, ensure_ascii=False, default=str)\n",
        "    print(f\"Log salvo em {filename}\")"
      ],
      "metadata": {
        "id": "FrgFQMniac1g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lista de ferramentas disponíveis"
      ],
      "metadata": {
        "id": "fBc4MgGL32R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate\",\n",
        "            \"description\": \"Calculates a mathematical expression\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"expression\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The math expression to evaluate, e.g., '2 + 2'\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"expression\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"wikipedia_search\",\n",
        "            \"description\": \"Search for a term on Wikipedia\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The search query\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_planet_mass\",\n",
        "            \"description\": \"Get the mass of a planet in kg\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"planet\": {\"type\": \"string\", \"description\": \"The name of the planet, e.g., 'Mars'\"}\n",
        "                },\n",
        "                \"required\": [\"planet\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "available_functions = {\n",
        "    \"calculate\": calculate,\n",
        "    \"wikipedia_search\": wikipedia_search,\n",
        "    \"get_planet_mass\": get_planet_mass,\n",
        "}"
      ],
      "metadata": {
        "id": "OyU6_AKjqNkc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### definindo novo modelo"
      ],
      "metadata": {
        "id": "3Lg1fRTk38ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# novo modelo, atualizado com base nos modelos atuais disponíveis no site do groq\n",
        "def steve_magal(question):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the available tools to answer questions correctly.\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    # criando log manual para checagem posterior\n",
        "    save_log_to_json(messages)\n",
        "\n",
        "    print(f\"--- Pergunta: {question} ---\")\n",
        "\n",
        "    # criando instancia do cliente\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\", # modelo encontrado no site oficial do GROQ\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        print(\"--> O modelo decidiu usar ferramentas!\")\n",
        "\n",
        "        messages.append(response_message)\n",
        "\n",
        "        # add informação no log\n",
        "        save_log_to_json(messages)\n",
        "\n",
        "        # iterando sobre as chamadas\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            print(f\"--> Executando: {function_name} com args {function_args}\")\n",
        "\n",
        "            if function_name == \"calculate\":\n",
        "                function_response = function_to_call(expression=function_args.get(\"expression\"))\n",
        "            elif function_name == \"wikipedia_search\":\n",
        "                function_response = function_to_call(query=function_args.get(\"query\"))\n",
        "            elif function_name == \"get_planet_mass\":\n",
        "                function_response = function_to_call(planet=function_args.get(\"planet\"))\n",
        "\n",
        "            print(f\"--> Resultado: {function_response}\")\n",
        "\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )\n",
        "\n",
        "           # add informação no log\n",
        "            save_log_to_json(messages)\n",
        "\n",
        "        # segunda chamada do modelo, para verificação das respostas\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=messages,\n",
        "        )\n",
        "        # add informação no log\n",
        "        save_log_to_json(messages)\n",
        "        return second_response.choices[0].message.content\n",
        "\n",
        "    else:\n",
        "        return response_message.content"
      ],
      "metadata": {
        "id": "H0fw0TRIQKqw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### prompt corrigido"
      ],
      "metadata": {
        "id": "09ABewWJ3_cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt simplificado para evitar o erro de parser\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful and knowledgeable AI assistant.\n",
        "Answer the user's questions correctly and concisely.\n",
        "If the question is complex, explain your reasoning step-by-step before providing the final answer.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "IFqrur_L8e1x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### chamada do modelo"
      ],
      "metadata": {
        "id": "wLcZlamk4CE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = steve_magal(\"What is the mass of earth times 5?\")\n",
        "print(f\"\\nRESPOSTA FINAL:\\n{res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigVUSaZTpQO",
        "outputId": "7ec82b0b-62d5-4f75-bb7b-88a23b2620eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log salvo em historico_conversa.json\n",
            "--- Pergunta: What is the mass of earth times 5? ---\n",
            "--> O modelo decidiu usar ferramentas!\n",
            "Log salvo em historico_conversa.json\n",
            "--> Executando: calculate com args {'expression': '5.972e24 * 5'}\n",
            "--> Resultado: 2.9860000000000004e+25\n",
            "Log salvo em historico_conversa.json\n",
            "Log salvo em historico_conversa.json\n",
            "\n",
            "RESPOSTA FINAL:\n",
            "The mass of the Earth is approximately \\(5.972 \\times 10^{24}\\) kg.  \n",
            "Multiplying that by 5 gives:\n",
            "\n",
            "\\[\n",
            "5.972 \\times 10^{24}\\ \\text{kg} \\times 5 \\approx 2.986 \\times 10^{25}\\ \\text{kg}.\n",
            "\\]\n",
            "\n",
            "So, the result is roughly **\\(2.986 \\times 10^{25}\\) kg**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lendo o histórico de \"pensamento\" do modelo"
      ],
      "metadata": {
        "id": "Y3-e7M0_q8MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ler_log(nome_arquivo=\"historico_conversa.json\"):\n",
        "    try:\n",
        "        with open(nome_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
        "            dados = json.load(f)\n",
        "            print(json.dumps(dados, indent=4, ensure_ascii=False))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Arquivo de log não encontrado. Rode o agente primeiro.\")\n",
        "\n",
        "ler_log()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qZcccm0T-XS",
        "outputId": "78a5595d-ccfe-4a21-e425-3ae1c422d7c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are a helpful assistant. Use the available tools to answer questions correctly.\"\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"What is the mass of earth times 5?\"\n",
            "    },\n",
            "    \"ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='fc_fe6cf9fe-8554-44d1-abd8-9bd5b42beab9', function=Function(arguments='{\\\"expression\\\":\\\"5.972e24 * 5\\\"}', name='calculate'), type='function')], reasoning='The user: \\\"What is the mass of earth times 5?\\\" They want mass of Earth multiplied by 5. We need to compute that. Mass of Earth: about 5.972 × 10^24 kg. Times 5: 2.986 × 10^25 kg. We could give that. Maybe use calculate tool? But it\\\\'s a simple multiplication. Could use the function? The calculate function expects an expression. Let\\\\'s use it.')\",\n",
            "    {\n",
            "        \"tool_call_id\": \"fc_fe6cf9fe-8554-44d1-abd8-9bd5b42beab9\",\n",
            "        \"role\": \"tool\",\n",
            "        \"name\": \"calculate\",\n",
            "        \"content\": \"2.9860000000000004e+25\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### executando o agente em loop"
      ],
      "metadata": {
        "id": "plDoRcLM3Yv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# função steve_magal alterada para retirar a \"memória inicial\"\n",
        "# e permitir a passagem de informação através das interações em loop\n",
        "\n",
        "def novo_steve_magal(messages):\n",
        "\n",
        "    save_log_to_json(messages)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        print(\"   [Pensamento]: O modelo decidiu usar ferramentas!\")\n",
        "        messages.append(response_message)\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            print(f\"   [Ação]: Executando {function_name} com args {function_args}\")\n",
        "\n",
        "            if function_name == \"calculate\":\n",
        "                function_response = function_to_call(expression=function_args.get(\"expression\"))\n",
        "            elif function_name == \"wikipedia_search\":\n",
        "                function_response = function_to_call(query=function_args.get(\"query\"))\n",
        "            elif function_name == \"get_planet_mass\":\n",
        "                function_response = function_to_call(planet=function_args.get(\"planet\"))\n",
        "\n",
        "            print(f\"   [Observação]: Resultado: {function_response}\")\n",
        "\n",
        "            messages.append({\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": str(function_response),\n",
        "            })\n",
        "            save_log_to_json(messages)\n",
        "\n",
        "        return \"CONTINUE\"\n",
        "\n",
        "    else:\n",
        "        return response_message.content"
      ],
      "metadata": {
        "id": "QQapROlbtuic"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loop_de_raciocinio(question, max_steps=10):\n",
        "    print(f\"--- Iniciando tarefa complexa: {question} ---\")\n",
        "\n",
        "    # \"memória\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use tools step-by-step to answer.\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    step = 0\n",
        "    while step < max_steps:\n",
        "        step += 1\n",
        "        print(f\"\\n>> Passo de Raciocínio {step}\")\n",
        "\n",
        "        # agora a função recebe a \"memória\" da interação passada\n",
        "        resultado = novo_steve_magal(messages)\n",
        "\n",
        "        if resultado == \"CONTINUE\":\n",
        "            continue\n",
        "        else:\n",
        "            return resultado\n",
        "\n",
        "    return \"Limite de passos atingido sem resposta final.\"\n",
        "\n",
        "pergunta = \"What is the mass of Earth plus the mass of Mars, and multiply the result by 2?\"\n",
        "resposta_final = loop_de_raciocinio(pergunta)\n",
        "\n",
        "print(f\"\\nRESPOSTA FINAL DO AGENTE:\\n{resposta_final}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7vU2u3B4wkI",
        "outputId": "4d4b2981-7e77-4687-c616-f3d02d998aa8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando tarefa complexa: What is the mass of Earth plus the mass of Mars, and multiply the result by 2? ---\n",
            "\n",
            ">> Passo de Raciocínio 1\n",
            "Log salvo em historico_conversa.json\n",
            "   [Pensamento]: O modelo decidiu usar ferramentas!\n",
            "   [Ação]: Executando get_planet_mass com args {'planet': 'Earth'}\n",
            "   [Observação]: Resultado: 5.972e+24\n",
            "Log salvo em historico_conversa.json\n",
            "\n",
            ">> Passo de Raciocínio 2\n",
            "Log salvo em historico_conversa.json\n",
            "   [Pensamento]: O modelo decidiu usar ferramentas!\n",
            "   [Ação]: Executando get_planet_mass com args {'planet': 'Mars'}\n",
            "   [Observação]: Resultado: 6.39e+23\n",
            "Log salvo em historico_conversa.json\n",
            "\n",
            ">> Passo de Raciocínio 3\n",
            "Log salvo em historico_conversa.json\n",
            "\n",
            "RESPOSTA FINAL DO AGENTE:\n",
            "The combined mass of Earth and Mars is approximately \\(6.611 \\times 10^{24}\\,\\text{kg}\\).  \n",
            "Multiplying that result by 2 gives about:\n",
            "\n",
            "\\[\n",
            "1.322 \\times 10^{25}\\;\\text{kg}\n",
            "\\]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_final = loop_de_raciocinio(input(\"Faça uma pergunta ao Steve Magal: \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJC-Q2AdSIlc",
        "outputId": "9fc0069a-5797-4fb8-e61d-3cf04572d057"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faça uma pergunta ao Steve Magal: Consegue me responder em português?\n",
            "--- Iniciando tarefa complexa: Consegue me responder em português? ---\n",
            "\n",
            ">> Passo de Raciocínio 1\n",
            "Log salvo em historico_conversa.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AqZj_8EW9qS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
