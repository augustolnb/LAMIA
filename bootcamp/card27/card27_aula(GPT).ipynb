{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## parte 1"
      ],
      "metadata": {
        "id": "cAjGP0MDoLEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46QD9TKSk9g5",
        "outputId": "3cd6a0ac-437a-43c1-c253-768f244fd0e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W2hmlLBmj_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06d4a3b-7e7a-408d-96bc-aada9c63a1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-13 16:44:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-13 16:44:25 (28.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "RabC-yFkLXnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tamanho do dataset: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQywSta5Lj_X",
        "outputId": "9f108b0c-6c4d-4407-a751-e01e479599ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do dataset: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCTS9CihLvGP",
        "outputId": "612cb018-3d7d-4a38-e282-d0255fcc1cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJ4KVKsLzET",
        "outputId": "bcc6d072-5233-4fe2-c0dd-246a6dc5430b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "YY4srhApMYLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('Hello mi amigo'))\n",
        "print(decode(encode('Hello mi amigo')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yAIe51xMlBR",
        "outputId": "dbe90200-b72e-44e7-94d5-f94d64caa6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 1, 51, 47, 1, 39, 51, 47, 45, 53]\n",
            "Hello mi amigo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da1AnYl6Mwmg",
        "outputId": "fd0553af-0266-4ee8-b808-c457088dc60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "yODkB3HDNi2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjR2sFcfOCja",
        "outputId": "b659ddfe-87ef-495e-bb27-3bc177848abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Para a entrada {context} a saída é: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbjw43KxOG7n",
        "outputId": "d0f05379-0006-4334-d5ab-de40dfeaebe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para a entrada tensor([18]) a saída é: 47\n",
            "Para a entrada tensor([18, 47]) a saída é: 56\n",
            "Para a entrada tensor([18, 47, 56]) a saída é: 57\n",
            "Para a entrada tensor([18, 47, 56, 57]) a saída é: 58\n",
            "Para a entrada tensor([18, 47, 56, 57, 58]) a saída é: 1\n",
            "Para a entrada tensor([18, 47, 56, 57, 58,  1]) a saída é: 15\n",
            "Para a entrada tensor([18, 47, 56, 57, 58,  1, 15]) a saída é: 47\n",
            "Para a entrada tensor([18, 47, 56, 57, 58,  1, 15, 47]) a saída é: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out\n",
        "\n",
        "xb, yb = get_bacth('train')\n",
        "print(\"Entradas:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"Saidas:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"--------\")\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"Para a entrada {context} a saída é: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8mLoDybOWox",
        "outputId": "f8417b04-d354-46c3-c446-4d57a86fad52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entradas:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Saidas:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "--------\n",
            "Para a entrada tensor([24]) a saída é: 43\n",
            "Para a entrada tensor([24, 43]) a saída é: 58\n",
            "Para a entrada tensor([24, 43, 58]) a saída é: 5\n",
            "Para a entrada tensor([24, 43, 58,  5]) a saída é: 57\n",
            "Para a entrada tensor([24, 43, 58,  5, 57]) a saída é: 1\n",
            "Para a entrada tensor([24, 43, 58,  5, 57,  1]) a saída é: 46\n",
            "Para a entrada tensor([24, 43, 58,  5, 57,  1, 46]) a saída é: 43\n",
            "Para a entrada tensor([24, 43, 58,  5, 57,  1, 46, 43]) a saída é: 39\n",
            "Para a entrada tensor([44]) a saída é: 53\n",
            "Para a entrada tensor([44, 53]) a saída é: 56\n",
            "Para a entrada tensor([44, 53, 56]) a saída é: 1\n",
            "Para a entrada tensor([44, 53, 56,  1]) a saída é: 58\n",
            "Para a entrada tensor([44, 53, 56,  1, 58]) a saída é: 46\n",
            "Para a entrada tensor([44, 53, 56,  1, 58, 46]) a saída é: 39\n",
            "Para a entrada tensor([44, 53, 56,  1, 58, 46, 39]) a saída é: 58\n",
            "Para a entrada tensor([44, 53, 56,  1, 58, 46, 39, 58]) a saída é: 1\n",
            "Para a entrada tensor([52]) a saída é: 58\n",
            "Para a entrada tensor([52, 58]) a saída é: 1\n",
            "Para a entrada tensor([52, 58,  1]) a saída é: 58\n",
            "Para a entrada tensor([52, 58,  1, 58]) a saída é: 46\n",
            "Para a entrada tensor([52, 58,  1, 58, 46]) a saída é: 39\n",
            "Para a entrada tensor([52, 58,  1, 58, 46, 39]) a saída é: 58\n",
            "Para a entrada tensor([52, 58,  1, 58, 46, 39, 58]) a saída é: 1\n",
            "Para a entrada tensor([52, 58,  1, 58, 46, 39, 58,  1]) a saída é: 46\n",
            "Para a entrada tensor([25]) a saída é: 17\n",
            "Para a entrada tensor([25, 17]) a saída é: 27\n",
            "Para a entrada tensor([25, 17, 27]) a saída é: 10\n",
            "Para a entrada tensor([25, 17, 27, 10]) a saída é: 0\n",
            "Para a entrada tensor([25, 17, 27, 10,  0]) a saída é: 21\n",
            "Para a entrada tensor([25, 17, 27, 10,  0, 21]) a saída é: 1\n",
            "Para a entrada tensor([25, 17, 27, 10,  0, 21,  1]) a saída é: 54\n",
            "Para a entrada tensor([25, 17, 27, 10,  0, 21,  1, 54]) a saída é: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebSR0ylmPV-e",
        "outputId": "eb07ccfe-52e4-4c68-97c3-881c367c7f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "m = m.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5987FfxRPlqF",
        "outputId": "02fc8186-1320-45e2-9357-c19e97441194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ZPLmnmUFQMv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "1QCbP93cd0zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNh_sBrlS6p_",
        "outputId": "6e049ca0-564a-4987-cbad-c7e8edea7e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4617247581481934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kqz8sYxTy1Z",
        "outputId": "c7f5d74c-de01-4127-91a3-0cfd6ca70c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Whole re.\n",
            "\n",
            "Sof todlisthinove toToren a fro sovins wes fromsse.\n",
            "I ire,qushindry cofer-hy celde tyomym\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## parte 2"
      ],
      "metadata": {
        "id": "DMO37olFoIBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "\ttext = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "\tdata = train_data if split == 'train' else val_data\n",
        "\tix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\tx = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\ty = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\tx, y = x.to(device), y.to(device)\n",
        "\treturn x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "\tout = {}\n",
        "\tm.eval()\n",
        "\tfor split in ['train', 'val']:\n",
        "\t\tlosses = torch.zeros(eval_iters)\n",
        "\t\tfor k in range(eval_iters):\n",
        "\t\t\tX, Y = get_batch(split)\n",
        "\t\t\tlogits, loss = m(X, Y)\n",
        "\t\t\tlosses[k] = loss.item()\n",
        "\t\tout[split] = losses.mean()\n",
        "\tm.train()\n",
        "\treturn out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\tdef __init__(self, vocab_size):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\tdef forward(self, idx, targets=None):\n",
        "\t\tlogits = self.token_embedding_table(idx)\n",
        "\n",
        "\t\tif targets is None:\n",
        "\t\t\tloss = None\n",
        "\t\telse:\n",
        "\t\t\tB, T, C = logits.shape\n",
        "\t\t\tlogits = logits.view(B*T, C)\n",
        "\t\t\ttargets = targets.view(B*T)\n",
        "\t\t\tloss = F.cross_entropy(logits, targets)\n",
        "\n",
        "\t\treturn logits, loss\n",
        "\n",
        "\tdef generate(self, idx, max_new_tokens):\n",
        "\t\tfor _ in range(max_new_tokens):\n",
        "\t\t\tlogits, loss = self(idx)\n",
        "\t\t\tlogits = logits[:, -1, :]\n",
        "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
        "\t\t\tidx_next = torch.multinomial(probs, num_samples=1)\n",
        "\t\t\tidx = torch.cat((idx, idx_next), dim=1)\n",
        "\t\treturn idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "m = m.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "\tif iter % eval_interval == 0:\n",
        "\t\tlosses = estimate_loss()\n",
        "\t\tprint(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "\txb, yb = get_batch(\"train\")\n",
        "\n",
        "\tlogits, loss = m(xb, yb)\n",
        "\toptimizer.zero_grad(set_to_none=True)\n",
        "\tloss.backward()\n",
        "\toptimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQfSmCzlTeNF",
        "outputId": "3bac6ea0-b89d-4ecb-a113-7a48867edca8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-14 13:40:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-06-14 13:40:02 (113 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "step 3000: train loss 2.4613, val loss 2.4897\n",
            "step 3300: train loss 2.4689, val loss 2.4793\n",
            "step 3600: train loss 2.4554, val loss 2.4919\n",
            "step 3900: train loss 2.4682, val loss 2.4906\n",
            "step 4200: train loss 2.4634, val loss 2.4882\n",
            "step 4500: train loss 2.4563, val loss 2.4804\n",
            "step 4800: train loss 2.4557, val loss 2.4852\n",
            "step 5100: train loss 2.4605, val loss 2.4926\n",
            "step 5400: train loss 2.4614, val loss 2.4932\n",
            "step 5700: train loss 2.4684, val loss 2.4781\n",
            "step 6000: train loss 2.4651, val loss 2.4672\n",
            "step 6300: train loss 2.4586, val loss 2.4858\n",
            "step 6600: train loss 2.4696, val loss 2.4893\n",
            "step 6900: train loss 2.4482, val loss 2.4910\n",
            "step 7200: train loss 2.4522, val loss 2.4827\n",
            "step 7500: train loss 2.4559, val loss 2.4896\n",
            "step 7800: train loss 2.4423, val loss 2.4936\n",
            "step 8100: train loss 2.4592, val loss 2.4923\n",
            "step 8400: train loss 2.4537, val loss 2.4865\n",
            "step 8700: train loss 2.4567, val loss 2.4805\n",
            "step 9000: train loss 2.4510, val loss 2.4840\n",
            "step 9300: train loss 2.4535, val loss 2.4868\n",
            "step 9600: train loss 2.4609, val loss 2.4908\n",
            "step 9900: train loss 2.4530, val loss 2.4864\n",
            "step 10200: train loss 2.4458, val loss 2.4902\n",
            "step 10500: train loss 2.4637, val loss 2.4865\n",
            "step 10800: train loss 2.4474, val loss 2.4911\n",
            "step 11100: train loss 2.4600, val loss 2.4898\n",
            "step 11400: train loss 2.4519, val loss 2.4950\n",
            "step 11700: train loss 2.4588, val loss 2.4881\n",
            "step 12000: train loss 2.4433, val loss 2.4878\n",
            "step 12300: train loss 2.4604, val loss 2.4895\n",
            "step 12600: train loss 2.4548, val loss 2.4948\n",
            "step 12900: train loss 2.4629, val loss 2.4802\n",
            "step 13200: train loss 2.4607, val loss 2.5007\n",
            "step 13500: train loss 2.4614, val loss 2.4952\n",
            "step 13800: train loss 2.4599, val loss 2.4817\n",
            "step 14100: train loss 2.4607, val loss 2.4879\n",
            "step 14400: train loss 2.4525, val loss 2.4945\n",
            "step 14700: train loss 2.4619, val loss 2.4917\n",
            "\n",
            "\n",
            "\n",
            "CExthy brid owindakis b, bth\n",
            "\n",
            "Hiset bobe d e.\n",
            "S:\n",
            "O:\n",
            "IS:\n",
            "Folatanss:\n",
            "Wanthar u qur, vet?\n",
            "F dilasoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, bes iree s, hein lat Heriliovets, and Win nghire yombousel lind te l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentoul kn mopetelaves\n",
            "MP:\n",
            "\n",
            "Pl, d mothakleo Windo whth eisbyo touth dourive we higend t so mower; te\n",
            "\n",
            "AN ad nthrupirf s ar iris! m:\n",
            "\n",
            "Thiny aleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myound, be!\n",
            "KENoby ak\n",
            "Sadsal thes ghesthidin cour ay aney Iry ts I fr t ce.\n",
            "J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mathematical trick"
      ],
      "metadata": {
        "id": "2tOpg7lqwEsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch. manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S43kG9LwH4w",
        "outputId": "412b94a7-a199-4009-f33e-b6069776c654"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1]\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "8MTQfLcxxH2G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJZ6y-kYxgzH",
        "outputId": "3fc33839-d7ba-4926-ae96-eebf87d415ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815",
        "id": "n3aioqmPynD5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v3"
      ],
      "metadata": {
        "id": "lFIJ7lBwjrD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author:Andrei-Aksionov\n",
        "# https://github.com/karpathy/ng-video-lecture/blob/52201428ed7b46804849dea0b3ccf0de9df1a5c3/gpt.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRT6krPmxy3X",
        "outputId": "619854bc-e12d-4b1a-b9f8-0944fe31a475"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-14 17:54:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-14 17:54:54 (24.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "10.788929 M parameters\n",
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7600, val loss 1.9146\n",
            "step 1000: train loss 1.3903, val loss 1.5987\n",
            "step 1500: train loss 1.2644, val loss 1.5271\n",
            "step 2000: train loss 1.1835, val loss 1.4978\n",
            "step 2500: train loss 1.1233, val loss 1.4910\n",
            "step 3000: train loss 1.0718, val loss 1.4804\n",
            "step 3500: train loss 1.0179, val loss 1.5127\n",
            "step 4000: train loss 0.9604, val loss 1.5102\n",
            "step 4500: train loss 0.9125, val loss 1.5351\n",
            "step 4999: train loss 0.8589, val loss 1.5565\n",
            "\n",
            "But with prison, I will steal for the fimker.\n",
            "\n",
            "KING HENRY VI:\n",
            "To prevent it, as I love this country's cause.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "I thank bhop my follow. Walk ye were so?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "My lord, I hearison! Who may love me accurse\n",
            "Some chold or flights then men shows to great the cur\n",
            "Ye cause who fled the trick that did princely action?\n",
            "Take my captiving sound, althoughts thy crown.\n",
            "\n",
            "RICHMOND NE:\n",
            "God neit will he not make it wise this!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Worthy Prince forth from Lord Claudio!\n",
            "\n",
            "Lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnsjJEAx1lX7",
        "outputId": "8ada0f6e-c581-4387-eb22-756c8cfaca8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I am sound to do for a king sleep:\n",
            "I came to convert thy grief; and then be thieve\n",
            "My indictment state and heart my soldier;\n",
            "Some thy fable of life is flat, to woo.\n",
            "\n",
            "ESCALUS:\n",
            "Learn's is that, and but that thy, by edict.\n",
            "\n",
            "POLIXENES:\n",
            "Your tongue, my lord.\n",
            "If you did mean they will this bud most know two:\n",
            "if you wish met; but that they smoth were noted trainful\n",
            "doing the one, and they stand goods for\n",
            "minemen, know not at such receivity to me\n",
            "welcome tof what's seen men.\n",
            "\n",
            "Shepherd:\n",
            "Out of this, night, if thou!\n",
            "\n",
            "ESCALUS:\n",
            "What are the prince, happy neck of his passes.\n",
            "\n",
            "POMPHEY:\n",
            "Then what make, fit shore sound for some requish,\n",
            "short this he hath done; would afflict him the mock.\n",
            "\n",
            "ANGELO:\n",
            "Go weep, my lords. Come, come hither, thy absent,\n",
            "Show'd thy frail and mock, and sworn break thirt.\n",
            "\n",
            "POMPEY:\n",
            "Since may, while you be glad and so swift ere then\n",
            "come to seek me the flow sighting, so bald. Pray,\n",
            "such forth as I can be as old. Let me come, follow.\n",
            "\n",
            "BENVOLIO:\n",
            "Here comes bleed, Johve ajoy, bear a\n"
          ]
        }
      ]
    }
  ]
}